
# SCALE-AMP: Multi-Scale Attention Model for Functional Antimicrobial Peptide Prediction

**SCALE-AMP** is a deep learning framework designed for both binary classification of antimicrobial peptides (AMPs) and multi-label functional activity prediction. It combines the power of pre-trained protein embeddings from ESM-2 with advanced modules like SCConv (local feature extraction), ProMamba (sequential modeling), and LiteMLA (multi-scale linear attention).

---

## ðŸ§  Model Architecture

The overall architecture of SCALE-AMP integrates sequence embeddings and multi-scale attention modules to capture both global and local patterns:

```
Input Sequence (FASTA)
       â†“
  ESM-2 Embeddings
       â†“
+---------------------+
| SCConv (Local)      |
| ProMamba (State)    |
| LiteMLA (Attention) |
+---------------------+
       â†“
Shared Feature Representation
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Binary Head  â”‚ Multi-label â”‚
â”‚ (AMP vs Non) â”‚  Heads (k)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> A high-resolution version of this figure can be added in `assets/scale-amp-architecture.png`.

---

## ðŸ“ Project Structure

```
SCALE-AMP/
â”‚
â”œâ”€â”€ Module/                  # Custom model components
â”‚   â”œâ”€â”€ litemla.py          # LiteMLA: Multi-scale linear attention
â”‚   â”œâ”€â”€ ProMamba.py         # ProMambaBlock: protein state modeling
â”‚   â”œâ”€â”€ ScConv.py           # SCConv: local feature extractors
â”‚
â”œâ”€â”€ model.py                # Full SCALE-AMP model
â”œâ”€â”€ loss.py                 # Loss functions: WBCE, CASL, dual-task
â”œâ”€â”€ data.py                 # AMPDataset: dataset wrapper
â”‚
â”œâ”€â”€ train_amp.py            # Binary classification (AMP vs Non-AMP)
â”œâ”€â”€ train_mtl.py            # Multi-label classification (functional tasks)
```

---

## âš™ï¸ Environment and Dependencies

Install the required packages:

```bash
pip install torch torchvision torchaudio
pip install pandas scikit-learn tqdm
pip install esm   # Pretrained ESM-2 from Facebook AI
pip install iterative-stratification
```

Python â‰¥3.8 and PyTorch â‰¥1.12 are recommended.

---

## ðŸš€ Usage

### Binary Classification (AMP vs Non-AMP)

```bash
python train_amp.py \
  --data data/Benchmark/Stage-1/AMP.csv \
  --output_dir checkpoints_amp \
  --epochs 100 \
  --batch_size 16 \
  --lr 5e-5 \
  --gpu 0 \
  --folds 5
```

### Multi-Label Functional Classification

```bash
python train_mtl.py \
  --data data/Benchmark/Stage-2/MTL.csv \
  --output_dir checkpoints_mtl \
  --epochs 100 \
  --batch_size 16 \
  --lr 5e-5 \
  --gpu 0 \
  --folds 5
```

---

## ðŸ“Š Benchmark Results

| Model       | Macro-F1 | Macro-AUC | Hamming Loss |
|-------------|----------|-----------|---------------|
| SCALE-AMP   | 0.473    | 0.826     | 0.124         |
| Deep-Amppred| 0.440    | 0.798     | 0.136         |

> Results based on 5-fold cross-validation on the benchmark dataset.

---

## ðŸ“¥ Example Input / Output

**Input:**
```
MKWVTFISLLFLFSSAYS
```

**Predicted Output:**
```
[Antibacterial: 0.91, Antifungal: 0.73, Anticancer: 0.12, ...]
```

---

## ðŸ“š Citation

If you use SCALE-AMP in your research, please cite:

```bibtex
@article{your2025scaleamp,
  title={SCALE-AMP: Multi-Scale Attention Model for Functional Antimicrobial Peptide Prediction},
  author={Your Name and Collaborators},
  journal={Bioinformatics},
  year={2025}
}
```

---

For issues, please contact the author or submit a GitHub issue.
